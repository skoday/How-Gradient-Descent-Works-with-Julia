{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc572d7a-b490-4201-8994-4bcce01b3654",
   "metadata": {},
   "source": [
    "# Gradicent Descent\n",
    "\n",
    "### Introduction\n",
    "In the general proccess of optimización multivaraible functions are used. Using different models to incrementally improve a design point until some convergence criterion is met.\n",
    "\n",
    "One method, local method, choose a direction to descend and at each iteration chooses a step size, this withing a region where the local method is believed to be valid.\n",
    "\n",
    "But on the other hand there are some other methods,frist order methos, second order methods, but the present method in discussión belong to the fist order set of methos.\n",
    "\n",
    "The idea of first methos rely in the fact that they only use the first derivate or gradient, and the same idea holds for grater order methods. But in the situation beign the since the derivates can engender infromation as to wich direction should be followed to optimize a problem it is useful, in this situation looking for a minimum.\n",
    "\n",
    "# Gradieent Descend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d745e1-45a8-48a8-aeca-b8f5a0a189a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: \"(2 * x)\" is not a valid function argument name around In[3]:7",
     "output_type": "error",
     "traceback": [
      "syntax: \"(2 * x)\" is not a valid function argument name around In[3]:7",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[3]:7"
     ]
    }
   ],
   "source": [
    "abstract type DescentMethod end\n",
    "\n",
    "struct GradientDescent <: DescentMethod\n",
    "  α\n",
    "end\n",
    "\n",
    "init!(M::GradientDescent, f, ∇f, x) = M\n",
    "\n",
    "function step!(M::GradientDescent, f, ∇f, x)\n",
    "  α, g = M.α, ∇f(x)\n",
    "  return x - α*g\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9cb4305-7990-4d9a-885d-ae44e5593a8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "StackOverflowError:",
     "output_type": "error",
     "traceback": [
      "StackOverflowError:",
      "",
      "Stacktrace:",
      " [1] GradientDescent(α::Float64) (repeats 79984 times)",
      "   @ Main ./In[2]:12"
     ]
    }
   ],
   "source": [
    "# Definición del tipo abstracto para el método de descenso\n",
    "abstract type DescentMethod end\n",
    "\n",
    "# Definición del tipo específico GradientDescent que hereda de DescentMethod\n",
    "struct GradientDescent <: DescentMethod\n",
    "    α\n",
    "end\n",
    "\n",
    "# Constructor para inicializar un objeto GradientDescent\n",
    "# En este caso, el constructor solo asigna el valor de α\n",
    "function GradientDescent(α)\n",
    "    return GradientDescent(α)\n",
    "end\n",
    "\n",
    "# Función cuadrática simple\n",
    "f(x) = x^2\n",
    "\n",
    "# Gradiente de la función cuadrática\n",
    "∇f(x) = 2x\n",
    "\n",
    "# Inicialización del método de descenso de gradiente\n",
    "α = 0.1  # Tasa de aprendizaje\n",
    "descent_method = GradientDescent(α)\n",
    "\n",
    "# Punto inicial para la optimización\n",
    "x_initial = 5.0\n",
    "\n",
    "# Número de iteraciones\n",
    "num_iterations = 10\n",
    "\n",
    "# Optimización usando el método de descenso de gradiente\n",
    "current_x = x_initial\n",
    "for i in 1:num_iterations\n",
    "    current_x = step!(descent_method, f, ∇f, current_x)\n",
    "    println(\"Iteración $i: x = $current_x, f(x) = $(f(current_x))\")\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
